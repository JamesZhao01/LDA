{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEXIS NLP and ML Notebook\n",
    "\n",
    "This notebook consists of all the experiments that James Zhao performed for the Hexis project in Spring 2021. \n",
    "The first cell consists of initialization for methods for LDA and plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "\n",
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def plot(matrix):\n",
    "    plt.imshow(np.array(matrix))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_matrixes(matrixes, labels, cols, rows):\n",
    "    if cols * rows > 1:\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(5,rows* 4 * 0.4))\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            fig.add_subplot(ax)\n",
    "            ax.imshow(matrixes[i])\n",
    "            ax.set_title(labels[i] if len(labels[i]) < 15 else labels[i][:15])\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    else:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(matrixes[0])\n",
    "        ax.set_title(labels[0] if len(labels[0]) < 15 else labels[0][:15])\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Model Creation. \n",
    "\n",
    "This cell loads data based on a root directory. Classes are assigned according to the subdirectories of the root directory, and this cell loads each file in each subdirectory. The following cell will run the LDA model with the given parameters and print out each respective topic with their respective top-10 words and weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# raw data of files\n",
    "files = []\n",
    "# file names of documents\n",
    "file_names = []\n",
    "\n",
    "# root directory of dataset\n",
    "root_dir = \"./corpus_05_30\"\n",
    "\n",
    "dirs = []\n",
    "# all possible classes (innermost sub-directory)\n",
    "classes = []\n",
    "\n",
    "# obtain all classes\n",
    "for w_root, w_dirs, w_files in os.walk(root_dir):\n",
    "    if len(w_dirs) == 0:\n",
    "        dirs.append(w_root)\n",
    "        classes.append(os.path.basename(w_root))\n",
    "print(f\"classes: {classes}\")\n",
    "\n",
    "dir_index = range(len(dirs))\n",
    "dirs = [dirs[i] for i in dir_index]\n",
    "\n",
    "# labels of each document\n",
    "labels = []\n",
    "\n",
    "for directory in dirs:\n",
    "    path = directory\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file), \"r\", encoding='utf-8') as infile:\n",
    "            files.append(infile.read())\n",
    "            file_names.append(file)\n",
    "            labels.append(os.path.basename(path))\n",
    "\n",
    "# apply processing to all docs\n",
    "processed_docs = [preprocess(i) for i in files]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Tf-idf dictionaries/corpuses, not used b/c of worse performance\n",
    "# tfidf = models.TfidfModel(bow_corpus)\n",
    "# corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "print(f\"length of corpus: {len(processed_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(\n",
    "    bow_corpus, num_topics=10, id2word=dictionary, passes=10, workers=4, minimum_probability=0)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying LDA model to each document\n",
    "\n",
    "This cell applies the LDA model to each document, turning them into a n-length vector, where n is the # of chosen topics. It also identifies the dominant topic of each document, as well as the document that is most representative for each n topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply lda model to all documents\n",
    "results = lda_model_tfidf[bow_corpus]\n",
    "dominant_topics = []\n",
    "\n",
    "# get most significant topic of each document:\n",
    "for i, row_list in enumerate(results):\n",
    "    row = sorted(row_list, key=lambda x: (x[1]), reverse=True)\n",
    "    topic_num, prop_topic = row[0]\n",
    "    wp = lda_model_tfidf.show_topic(topic_num)\n",
    "    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "    dominant_topics.append((i, topic_num, prop_topic, topic_keywords))\n",
    "    \n",
    "# results = [] for pp_file in processed_docs]\n",
    "results = [[i[1] for i in result] for result in results]\n",
    "results = np.array(results)\n",
    "\n",
    "# normalized vectors\n",
    "norms = np.linalg.norm(results, axis=1)\n",
    "normalized = results / norms.reshape(-1, 1)\n",
    "\n",
    "# get most representative topic from each topic\n",
    "most_representative = []\n",
    "for i in range(lda_model_tfidf.num_topics):\n",
    "    docs = [item for item in dominant_topics if item[1] == i]\n",
    "    if len(docs) == 0:\n",
    "        most_representative.append(None)\n",
    "        continue\n",
    "    else:\n",
    "        docs = sorted(docs, key=lambda x: (x[2]))\n",
    "        most_representative.append(docs[0])\n",
    "print(most_representative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "The following cells provide useful visualizations. The first constructs word clouds of each of the topics, the next one provides \n",
    "more specific details of each topic using an LDA topic visualization library, the third provides a visualization of the distribution of \n",
    "document lengths, and the fourth provides a visualization of each topic's word weight and respective word frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# wordclouds\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()] \n",
    "  \n",
    "\n",
    "cloud = WordCloud(stopwords=gensim.parsing.preprocessing.STOPWORDS,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i % 10],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model_tfidf.show_topics(num_topics = 15, formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(7, 7), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(topics):\n",
    "        fig.add_subplot(ax)\n",
    "        topic_words = dict(topics[i][1])\n",
    "        cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "        plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lda vis\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "gensimvis.prepare(lda_model_tfidf, bow_corpus, dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in processed_docs]\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.grid()\n",
    "ax.hist(doc_lens, bins = 100, color='navy')\n",
    "ax.text(0.8, 0.1, \"Mean   : \" + str(round(np.mean(doc_lens))), transform=ax.transAxes)\n",
    "ax.text(0.8, 0.15, \"Median : \" + str(round(np.median(doc_lens))), transform=ax.transAxes)\n",
    "ax.text(0.8, 0.2, \"Stdev   : \" + str(round(np.std(doc_lens))), transform=ax.transAxes)\n",
    "ax.text(0.8, 0.25, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))), transform=ax.transAxes)\n",
    "ax.text(0.8, 0.3, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))), transform=ax.transAxes)\n",
    "\n",
    "ax.set(ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# word counts of topic keywords\n",
    "from collections import Counter\n",
    "topics = lda_model_tfidf.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in processed_docs for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, int(counter[word])])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(8, 15), sharey=True, dpi=100)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    selected = np.array([item for item in out if item[1] == i])\n",
    "    xax = np.arange(len(selected))\n",
    "    counts = selected[:,3].astype(int)\n",
    "    weights = selected[:,2].astype(float)\n",
    "    \n",
    "    ax.bar(x=xax, height=counts, color=cols[i], width=0.6, alpha=0.6, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x=xax, height=weights, color='purple', width=0.25, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylabel('Weights', color='purple')\n",
    "    ax_twin.set_ylim(0, 0.1);\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=True)\n",
    "    ax.set_xticks(ticks=np.arange(0, 10, 1))\n",
    "    ax.set_xticklabels(labels=selected[:,0], rotation=90)\n",
    "    \n",
    "#     ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "sample = random.sample(list(zip(file_names, results)), 10)\n",
    "sel = [item[0] for item in sample]\n",
    "sel_res = [item[1] for item in sample]\n",
    "\n",
    "max_len = max([len(title) for title in sel])\n",
    "print(f\"{'Title' : <{max_len}} \" + \" \".join(f\"T{i:<4d}\" for i in range(10)))\n",
    "for i in range(len(sel)):\n",
    "    fil = sel[i]\n",
    "    res = sel_res[i]\n",
    "    print(f\"{fil.ljust(max_len)[:max_len]} \" + \" \".join(f\"{res[i]:0.3f}\" if res[i] > 1e-3 else \"-----\" for i in range(10)))\n",
    "    \n",
    "    \n",
    "print(f\"# samples: {len(results)}, # classes: {len(classes)}\")\n",
    "for item in classes:\n",
    "    print(f\"Class: {item} Count: {len([sample for sample in labels if sample == item])}\")\n",
    "    \n",
    "\n",
    "# stopword_list = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "# for i in range(5):\n",
    "#     print(f\"{' '.join(stopword_list[i*5:i*5 + 5])} \")\n",
    "    \n",
    "# sentence = \"Me and Joe carefully walked to the police station.\"\n",
    "# print(\"Original: \" + sentence)\n",
    "# sentence = gensim.utils.simple_preprocess(sentence)\n",
    "# print(\"Tokenized: \" + str(sentence))\n",
    "# sentence = [word for word in sentence if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3]\n",
    "# print(\"Removed Stopwords: \" + str(sentence))\n",
    "# sentence = [lemmatize_stemming(word) for word in sentence]\n",
    "# print(\"Lemmatized: \" + str(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme exploration based on Topic Vectors\n",
    "\n",
    "The following cell attempts to find some structure between the themes/labels based on the topics of the documents belonging to each theme. \n",
    "First, a covariance matrix of each label is constructed for each theme, and then the cosine distance between each covariance matrix\n",
    "visualizes which themese tend to have similar topic activations. This distance matrix is also plotted. The following cell constructs\n",
    "a dendrogram uses hierarchical clustering to find topics that tended to be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Topic Graph for each Theme\n",
    "cov_matrixes = []\n",
    "for (idx, item) in enumerate(classes):\n",
    "    indexes = [i for i in range(len(labels)) if labels[i] == item]\n",
    "    selected_results = results[indexes]\n",
    "    cov = np.cov(selected_results.T)\n",
    "    cov_matrixes.append(cov)\n",
    "\n",
    "plot_matrixes(cov_matrixes, classes, 3, 4)\n",
    "\n",
    "cos = []\n",
    "for i in range(len(cov_matrixes)):\n",
    "    cos.append([0 for x in range(len(cov_matrixes))])\n",
    "    for j in range(len(cov_matrixes)):\n",
    "        cos[i][j] = distance.cosine(\n",
    "            cov_matrixes[i].reshape(-1), cov_matrixes[j].reshape(-1))\n",
    "plot_matrixes([np.array(cos)], [\"cossims\"], 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "dend = shc.dendrogram(shc.linkage(cos, method='ward'), labels=classes, leaf_rotation=-90, leaf_font_size=8)\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.show()\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Matrixes\n",
    "\n",
    "The following cell is used for computing distances between topic vectors for each document. Due to the computation being n^2, only run\n",
    "if a few documents are present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jensen_shannon = []\n",
    "for i in range(len(results)):\n",
    "    jensen_shannon.append([0 for x in range(len(results))])\n",
    "    for j in range(len(results)):\n",
    "        jensen_shannon[i][j] = distance.jensenshannon(\n",
    "            results[i], results[j])\n",
    "cos = []\n",
    "for i in range(len(results)):\n",
    "    cos.append([0 for x in range(len(results))])\n",
    "    for j in range(len(results)):\n",
    "        cos[i][j] = distance.cosine(\n",
    "            results[i], results[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(cos)\n",
    "plot(jensen_shannon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "\n",
    "The following cells attempts to run the K means algorithm to identify clusters among the topic vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(init = \"random\", n_clusters = 4, n_init=10, max_iter=300, random_state=42)\n",
    "kmeans.fit(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_\n",
    "# offsets = normalized - kmeans.cluster_centers_\n",
    "repeated = np.expand_dims(normalized, axis=1)\n",
    "repeated = np.repeat(repeated, repeats=4, axis=1)\n",
    "offsets = repeated - kmeans.cluster_centers_\n",
    "distances = np.linalg.norm(offsets, axis=2)\n",
    "mins = np.argmin(distances, axis=1)\n",
    "print(mins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "The following cells construct a KNN model to classify the documents based on their normalized topic vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(results, labels, test_size=0.3, random_state=1)\n",
    "model.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "predicted_index = np.array([classes.index(i) for i in predicted])\n",
    "successes = np.zeros(len(classes))\n",
    "totals = np.zeros(len(classes))\n",
    "for i in range(len(X_test)):\n",
    "    file = file_names[i]\n",
    "    label = y_test[i]\n",
    "    if classes.index(label) == predicted_index[i]:\n",
    "        successes[classes.index(label)] += 1\n",
    "    totals[classes.index(label)] += 1\n",
    "    print(f\"i:{i:02} file:{file[:30]}{' '*(30 - len(file))} label:{classes.index(label)} pred:{predicted_index[i]}\")\n",
    "print(f\"Theme-wise accuracy: {successes/totals}\")\n",
    "print(f\"Overall accuracy: {np.sum(successes)/np.sum(totals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(normalized.T)\n",
    "\n",
    "results_pca = np.matmul(normalized, principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# from mpl_toolkits import mplot3d\n",
    "# mapping = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)    \n",
    "\n",
    "mapping = np.array(tableau20)\n",
    "\n",
    "# colors = mapping[predicted_index]\n",
    "indexes = np.array([classes.index(item) for item in labels])\n",
    "colors = mapping[indexes]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter3D(results_pca[:, 0], results_pca[:, 1], results_pca[:, 2], c=colors);\n",
    "# plt.scatter(results_pca[:, 0], results_pca[:, 1], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "The following cells contain the experiment involving the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(result, labels, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "#     print('Topic: {} Word: {}'.format(idx, topic))\n",
    "# text_representation = tree.export_text(clf)\n",
    "# print(text_representation)\n",
    "\n",
    "# Print classes\n",
    "# for (idx, item) in enumerate(classes):\n",
    "#     print(idx, item)\n",
    "\n",
    "# fig = plt.figure(figsize=(200, 100))\n",
    "# _ = tree.plot_tree(clf, \n",
    "#                    feature_names=[\"Theme \" + str(i) for i in range(len(lda_model_tfidf.print_topics(-1)))],  \n",
    "#                    class_names=classes,\n",
    "#                    filled=True, fontsize=14)\n",
    "# fig.savefig(\"decistion_tree.png\", dpi=100)\n",
    "\n",
    "import graphviz\n",
    "# DOT data\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=[\"Theme \" + str(i) for i in range(len(lda_model_tfidf.print_topics(-1)))],  \n",
    "                                class_names=classes,\n",
    "                                filled=True)\n",
    "\n",
    "# Draw graph\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph\n",
    "graph.render(\"dt_viz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass SVM\n",
    "\n",
    "The following cells include a classification experiment using a SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(results, labels, test_size=0.3, random_state=1)\n",
    "model = svm.SVC(kernel=\"rbf\").fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "for i in range(len(y_test)):\n",
    "    print(f\"test: {y_test[i]} | pred: {y_pred[i]}\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes \n",
    "\n",
    "The following involves a classification test using a Naive Bayes Classifier (partly to also get probability vectors of themes for each document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "data = results\n",
    "labels = labels\n",
    "encoded_labels = [classes.index(label) for label in labels]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(results, encoded_labels, test_size=0.3,random_state=109)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict_proba(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, gnb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDA",
   "language": "python",
   "name": "lda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
