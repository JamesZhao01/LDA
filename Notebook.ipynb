{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "\n",
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def plot(matrix):\n",
    "    plt.imshow(np.array(matrix))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "file_names = []\n",
    "dirs = [\"Basic Computer Skills\", \"Internet Skills\",\n",
    "        \"Microsoft Digital Literacy Course\", \"Microsoft Files\", \"Microsoft Office Videos\"]\n",
    "dir_index = [0, 1, 2, 3, 4]\n",
    "dirs = [dirs[i] for i in dir_index]\n",
    "labels = []\n",
    "\n",
    "for directory in dirs:\n",
    "    path = f\"./corpus/{directory}\"\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file), \"r\", encoding='utf-8') as infile:\n",
    "            files.append(infile.read())\n",
    "            file_names.append(file)\n",
    "            labels.append(directory)\n",
    "processed_docs = [preprocess(i) for i in files]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_docs[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(file_names)):\n",
    "#     print(f\"i: {i}, file: {file_names[i]}\")\n",
    "print(len(labels), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(\n",
    "    corpus_tfidf, num_topics=100, id2word=dictionary, passes=25, workers=4, minimum_probability=0)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [lda_model_tfidf[dictionary.doc2bow(\n",
    "    pp_file)] for pp_file in processed_docs]\n",
    "results = [[i[1] for i in result] for result in results]\n",
    "results = np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theme Analysis\n",
    "means = np.mean(results, axis=0)\n",
    "\n",
    "print(means)\n",
    "\n",
    "threshold = 1e-4\n",
    "relevant_indexes = np.array(np.where(means >= threshold))[0]\n",
    "print(relevant_indexes)\n",
    "\n",
    "filtered = results[:,relevant_indexes]\n",
    "topics = np.array(lda_model_tfidf.print_topics(-1))\n",
    "selected_topics = topics[relevant_indexes][:, 1]\n",
    "print(selected_topics[6])\n",
    "print(filtered)\n",
    "\n",
    "results = filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(results)\n",
    "norms = np.linalg.norm(results, axis=1)\n",
    "normalized = results / norms.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistanceMatrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jensen_shannon = []\n",
    "for i in range(len(results)):\n",
    "    jensen_shannon.append([0 for x in range(len(results))])\n",
    "    for j in range(len(results)):\n",
    "        jensen_shannon[i][j] = distance.jensenshannon(\n",
    "            results[i], results[j])\n",
    "cos = []\n",
    "for i in range(len(results)):\n",
    "    cos.append([0 for x in range(len(results))])\n",
    "    for j in range(len(results)):\n",
    "        cos[i][j] = distance.cosine(\n",
    "            results[i], results[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(cos)\n",
    "plot(jensen_shannon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(init = \"random\", n_clusters = 4, n_init=10, max_iter=300, random_state=42)\n",
    "kmeans.fit(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_\n",
    "# offsets = normalized - kmeans.cluster_centers_\n",
    "repeated = np.expand_dims(normalized, axis=1)\n",
    "repeated = np.repeat(repeated, repeats=4, axis=1)\n",
    "offsets = repeated - kmeans.cluster_centers_\n",
    "distances = np.linalg.norm(offsets, axis=2)\n",
    "mins = np.argmin(distances, axis=1)\n",
    "print(mins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(normalized,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(results)\n",
    "predicted_index = np.array([dirs.index(i) for i in predicted])\n",
    "successes = np.zeros(len(dirs))\n",
    "totals = np.zeros(len(dirs))\n",
    "for i in range(len(results)):\n",
    "    file = file_names[i]\n",
    "    label = labels[i]\n",
    "    if dirs.index(label) == predicted_index[i]:\n",
    "        successes[dirs.index(label)] += 1\n",
    "    totals[dirs.index(label)] += 1\n",
    "    print(f\"i:{i:02} file:{file[:30]}{' '*(30 - len(file))} label:{dirs.index(label)} pred:{predicted_index[i]}\")\n",
    "print(successes/totals)\n",
    "print(np.sum(successes)/np.sum(totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "covariance_matrix = np.cov(normalized.T)\n",
    "eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)\n",
    "projection_matrix = (eigen_vectors.T[:][:3]).T\n",
    "print(eigen_values)\n",
    "results_pca = results.dot(projection_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "mapping = np.array([\"blue\", \"red\", \"yellow\", \"green\", \"orange\"])\n",
    "colors = mapping[predicted_index]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(results_pca[:, 0], results_pca[:, 1], results_pca[:, 2], c=colors);\n",
    "# plt.scatter(results_pca[:, 0], results_pca[:, 1], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(results, labels, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    print(f\"{y_test[i]} | {y_pred[i]}\")\n",
    "    \n",
    "print(\"asdfasdfsadf\")\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    print(f\"{y_train[i]} | {y_pred[i]}\")\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "text_representation = tree.export_text(clf)\n",
    "print(text_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(results, labels, test_size=0.3, random_state=1)\n",
    "model = svm.SVC(kernel=\"rbf\").fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "for i in range(len(y_test)):\n",
    "    print(f\"test: {y_test[i]} | pred: {y_pred[i]}\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDA",
   "language": "python",
   "name": "lda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
